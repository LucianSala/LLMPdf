{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing and querying multiple PDF file (papers/guides/documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variable.OpenAPI_key import OPENAI_API_KEY\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "import langchain\n",
    "import openai\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.2) #high level of precision for summarization\n",
    "def summ_pdfs(pdfs_folder):\n",
    "    summaries=[]\n",
    "    for pdf_file in glob.glob(pdfs_folder + \"/*.pdf\"):\n",
    "        loader = PyPDFLoader(pdf_file)\n",
    "        docs = loader.load_and_split()\n",
    "        chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "        summary = chain.run(docs)\n",
    "        print(\"Summary for PDF: \", pdf_file)\n",
    "        print(summary)\n",
    "        print(\"\\n\")\n",
    "        summaries.append(summary)\n",
    "    return summaries\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for PDF:  ./pdfs_folder\\2306.00989v1.pdf\n",
      " This paper introduces Hiera, a hierarchical vision transformer that is simpler and faster than its predecessors while still achieving high accuracy. It is evaluated on a variety of image and video recognition tasks and is shown to outperform the state-of-the-art models. Hiera is pretrained with a strong visual pretext task (MAE) and uses local and global attention within \"mask units\" for efficient and accurate training. It is also compatible with popular self-supervised tasks like masked image modeling. Hiera is compared to other vision transformers such as Convnext v2, Detectron2, Simmim, Cutmix, Mixup, and Places Database.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for PDF:  ./pdfs_folder\\DragYourGAN.pdf\n",
      " This paper presents DragGAN, a novel approach for interactive point-based manipulation of GAN-generated images. It consists of two components: a feature-based motion supervision and a point tracking approach. Through DragGAN, users can manipulate the pose, shape, expression, and layout of diverse object categories. It outperforms existing point tracking approaches such as RAFT and PIPs, and can be combined with GAN inversion techniques for real image editing. Quantitative and qualitative evaluations demonstrate the advantage of DragGAN over prior approaches in the tasks of image manipulation and point tracking.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summaries = summ_pdfs(\"./pdfs_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving summaries to a local file\n",
    "\n",
    "with open(\"summaries.txt\", \"w\") as f:\n",
    "    for summary in summaries:\n",
    "        f.write(summary + \"\\n\"*2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying PDF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFDirectoryLoader(\"./pdfs_folder/\")\n",
    "\n",
    "docs = loader.load()\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The method used to create the interactive image manipulation method is point-based manipulation, which allows users to click any number of handle points and target points on the image and the goal is to drive the handle points to reach their corresponding target points.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query for the Darg you GAN paper\n",
    "\n",
    "query = \"What is the method used to create the nteractive image manipulation method\"\n",
    "index.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
